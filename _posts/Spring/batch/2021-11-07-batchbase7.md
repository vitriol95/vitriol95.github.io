---
title: 스프링 배치 - 7 ItemWriter
date: 2021-11-07 12:00:00 +0900
categories: [Spring,batch]
tags: [spring,springboot,batch]
---

# ItemWriter

지난시간에 포스팅했던 **ItemReader**와 더불어 청크 지향 프로세스에 필수적인 ItemWriter에 대해 이야기할 예정이다.

## 기본 개념

청크 지향 프로세스의 최종단계로, processor 혹은 reader로 부터 받은 자료들을 Chunk단위로 받아 일괄 출력 작업을 위한 인터페이스에 속한다. **ItemReader**와 동일한 자료형들을 모두 지원한다.

<br/>

가지고있는 메서드는 다음 하나뿐이다. 따라서 람다식을 활용해 넘겨줄 수 있다. `void write(List<? extends T> items)`

<img src="/assets/img/batch/25.png">

당연히 Stream이 필요하며 다수의 구현체들이 Writer와 함께 구현되어있다. 이들중 DataBase와 관련된 ItemWriter들을 알아볼 것이다.

- JdbcBatchItemWriter
- JpaBatchItemWriter
- HibernateBatchItemWriter (참고만)

## JdbcBatchItemWriter

Reader와 마찬가지로 datasource를 지정하고, sql 속성에 실행할 쿼리를 설정하면 된다. 효율상의 이점을 위해 `JdbcTemplate`의 Batch기능을 활용하여 `bulk Insert/Update/Delete` 방식으로 데이터를 처리한다.
> insert는 하나만 나오지만 values가 줄줄이 이어지는 쿼리가 그것이다.

> insert into account values (1, 'a','b'), (2,'c','d'), (3,'e','f') ... 이런식의

<img src="/assets/img/batch/26.png">

### 코드 뜯어보기

그렇다면 실제 코드를 뜯어보며 이들이 어떻게 작동하고 있는지 확인해보자.

```java
@Override
public void write(final List<? extends T> items) throws Exception {

    if (!items.isEmpty()) {

        if (logger.isDebugEnabled()) {
            logger.debug("Executing batch with " + items.size() + " items.");
        }

        int[] updateCounts;

        if (usingNamedParameters) {
            if(items.get(0) instanceof Map && this.itemSqlParameterSourceProvider == null) {
                updateCounts = namedParameterJdbcTemplate.batchUpdate(sql, items.toArray(new Map[items.size()]));
            } else {
                SqlParameterSource[] batchArgs = new SqlParameterSource[items.size()];
                int i = 0;
                for (T item : items) {
                    batchArgs[i++] = itemSqlParameterSourceProvider.createSqlParameterSource(item);
                }
                updateCounts = namedParameterJdbcTemplate.batchUpdate(sql, batchArgs);
            }
        }
        else {
            updateCounts = namedParameterJdbcTemplate.getJdbcOperations().execute(sql, new PreparedStatementCallback<int[]>() {
                @Override
                public int[] doInPreparedStatement(PreparedStatement ps) throws SQLException, DataAccessException {
                    for (T item : items) {
                        itemPreparedStatementSetter.setValues(item, ps);
                        ps.addBatch();
                    }
                    return ps.executeBatch();
                }
            });
        }

        if (assertUpdates) {
            for (int i = 0; i < updateCounts.length; i++) {
                int value = updateCounts[i];
                if (value == 0) {
                    throw new EmptyResultDataAccessException("Item " + i + " of " + updateCounts.length
                            + " did not update any rows: [" + items.get(i) + "]", 1);
                }
            }
        }
    }
}
```

코드가 상당히 길지만 우리가 눈여겨보아야할 부분만 들어가보자. 가장먼저, List타입을 args로 받는것이다. 이 부분은 전부터 보아왔기에 그리 어색하지 않다.

<br/>

두번째로는 `usingNamedParameters`에 따른 분기가 나타나고 있는 것이다. 이변수는 무엇인가 찾아보니, 쿼리의 인자표시를 설정하는 항목에 해당한다. 이는 `afterPropertiesSet()`메서드에 등장한다.

```java
if (namedParameters.size() > 0) {
			if (parameterCount != namedParameters.size()) {
				throw new InvalidDataAccessApiUsageException("You can't use both named parameters and classic \"?\" placeholders: " + sql);
			}
			usingNamedParameters = true;
		}
		if (!usingNamedParameters) {
			Assert.notNull(itemPreparedStatementSetter, "Using SQL statement with '?' placeholders requires an ItemPreparedStatementSetter");
		}
```

결국, 쿼리를 작성할때 인자 표시가 : 이냐, ? 이냐에 따라 true, false값을 갖는 변수이다. ColumnMapped, BeanMapped인 경우 1번을 따라간다.

1. `insert into account (name, year) values (:name, :year)` 인경우에는 true에 해당하겠고 

2. `insert into account (name, year) values (?, ?)`처럼 바인딩 된 경우에는 false로 반환된다. 

다시 본문으로 돌아와서 어떤 경우에 해당하든 핵심은 `namedParameterJdbcTemplate.batchUpdate()` 및 `ps.addBatch()`를 통해 실제 실행될 쿼리를 업데이트시키고 마지막으로 `executeBatch()`를 실행하는 것이다.

### 예시 코드 

```java
@Slf4j
@RequiredArgsConstructor
@Configuration
public class JdbcBatchItemWriterJobConfiguration {

    private final JobBuilderFactory jobBuilderFactory;
    private final StepBuilderFactory stepBuilderFactory;
    private final DataSource dataSource; 

    private static final int chunkSize = 10;

    @Bean
    public Job jdbcBatchItemWriterJob() {
        return jobBuilderFactory.get("jdbcBatchItemWriterJob")
                .start(jdbcBatchItemWriterStep())
                .build();
    }

    @Bean
    public Step jdbcBatchItemWriterStep() {
        return stepBuilderFactory.get("jdbcBatchItemWriterStep")
                .<Customer, Customer>chunk(chunkSize)
                .reader(jdbcBatchItemWriterReader())
                .writer(jdbcBatchItemWriter())
                .build();
    }

    @Bean
    public JdbcCursorItemReader<Customer> jdbcBatchItemWriterReader() {
        return new JdbcCursorItemReaderBuilder<Customer>()
                .fetchSize(chunkSize)
                .dataSource(dataSource)
                .rowMapper(new BeanPropertyRowMapper<>(Customer.class))
                .sql("SELECT id, firstname, lastname  FROM customer")
                .name("jdbcBatchItemWriter")
                .build();
    }

    @Bean
    public JdbcBatchItemWriter<Customer> jdbcBatchItemWriter() {
        return new JdbcBatchItemWriterBuilder<Customer>()
                .dataSource(dataSource)
                .sql("insert into customer2(firstname, lastname) values (:firstname,:lastname)")
                .beanMapped()
                .build();
    }
}
```

만약 이를 .`columnMapped()`로 변경한다면 다음과 같은 코드가 작성될 것이다.

```java
new JdbcBatchItemWriterBuilder<Map<String, Object>>() 
    .columnMapped()
    .dataSource(this.dataSource)
    .sql("insert into customer2(firstname, lastname) values (:firstname,:lastname)")
    .build();
```

## JpaItemWriter

`EntityManagerFactory`를 주입받아 사용해야하며, 엔티티를 청크크기만큼 insert / merge 를 진행한뒤 `flush()`가 진행된다.
> 이때 `persist()` 옵션또한 주어지며, 이 값이 false이면 자동으로 merge()처리가 된다.

또한 이런 `@PersistenceUnit`인 `EntityManagerFactory`는 spring data jpa를 사용하면 자동으로 빈등록이 되므로 이를 주입받아 사용하기만 하면 된다.

<img src="/assets/img/batch/27.png">

### 코드 뜯어보기

```java
@Override
public void write(List<? extends T> items) {
    EntityManager entityManager = EntityManagerFactoryUtils.getTransactionalEntityManager(entityManagerFactory);
    if (entityManager == null) {
        throw new DataAccessResourceFailureException("Unable to obtain a transactional EntityManager");
    }
    doWrite(entityManager, items);
    entityManager.flush();
}

protected void doWrite(EntityManager entityManager, List<? extends T> items) {

    if (!items.isEmpty()) {
        long addedToContextCount = 0;
        for (T item : items) {
            if (!entityManager.contains(item)) {
                if(usePersist) {
                    entityManager.persist(item);
                }
                else {
                    entityManager.merge(item);
                }					
                addedToContextCount++;
            }
        }
        if (logger.isDebugEnabled()) {
            logger.debug(addedToContextCount + " entities " + (usePersist ? " persisted." : "merged."));
            logger.debug((items.size() - addedToContextCount) + " entities found in persistence context.");
        }
    }

}
```

`doWrite()` 메서드를 통해 보면 전달받은 아이템을 하나씩 돌아가며 `persist()` or `merge()`를 진행하고 있다. 이후 `write()`메서드의 마지막엔 `flush()`가 일어난다.

### 예시 코드 

```java
@Slf4j
@RequiredArgsConstructor
@Configuration
public class JpaItemWriterJobConfiguration {
    private final JobBuilderFactory jobBuilderFactory;
    private final StepBuilderFactory stepBuilderFactory;
    private final EntityManagerFactory entityManagerFactory;

    private static final int chunkSize = 10;

    @Bean
    public Job jpaItemWriterJob() {
        return jobBuilderFactory.get("jpaItemWriterJob")
                .start(jpaItemWriterStep())
                .build();
    }

    @Bean
    public Step jpaItemWriterStep() {
        return stepBuilderFactory.get("jpaItemWriterStep")
                .<Customer, Customer2>chunk(chunkSize)
                .reader(jpaItemWriterReader())
                .processor(jpaItemProcessor())
                .writer(jpaItemWriter())
                .build();
    }

    @Bean
    public JpaPagingItemReader<Customer> jpaItemWriterReader() {
        return new JpaPagingItemReaderBuilder<Customer>()
                .name("jpaItemWriterReader")
                .entityManagerFactory(entityManagerFactory)
                .pageSize(chunkSize)
                .queryString("SELECT c FROM Customer c")
                .build();
    }

    @Bean
    public ItemProcessor<Customer, Customer2> jpaItemProcessor() {
        return c -> new Customer2(c.getFirstname(), c.getLastname());
    }

    @Bean
    public JpaItemWriter<Customer2> jpaItemWriter() {
        JpaItemWriter<Customer2> jpaItemWriter = new JpaItemWriter<>();
        jpaItemWriter.setEntityManagerFactory(entityManagerFactory);
        return jpaItemWriter;
    }
}
```

# 출처

- [기억보다 기록을 - SSpring Batch 가이드 - ItemWriter](https://jojoldu.tistory.com/339?category=902551)
- [Spring Batch 강의 - 정수원님](https://www.inflearn.com/course/%EC%8A%A4%ED%94%84%EB%A7%81-%EB%B0%B0%EC%B9%98) 